{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a801d278-6b81-472b-9cbb-8fb0f8213172",
   "metadata": {},
   "source": [
    "# Mining & Modelling Character Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb354d5-34b5-41d3-acc2-29e486d12f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# import io\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a421c37-8d92-46d0-b14d-de1a31a63574",
   "metadata": {},
   "source": [
    "Paper Reference [Here](https://math.ryerson.ca/~abonato/papers/CharacterNetworks_WAW_Aug1_BDAEGH.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e73969d-37cc-4079-a88a-f470c3e30c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "data_path = './data/data.csv'\n",
    "names_df_path = './data/ontario_names_list_1917-2019.csv'\n",
    "stop = stopwords.words('english')\n",
    "additional_sw = [\n",
    "    'well', 'would', 'never', 'latitude', 'longitude', 'wonderland', 'wonder', 'adventure', 'adventures', 'chapter',\n",
    "    'please', 'maam', 'drink', 'think', 'sink', 'come', 'foot', 'right', 'thats', 'too', 'itll', 'tell', 'table', 'long',\n",
    "    'tale', 'test', 'said', 'held', 'crab', 'next', 'sure', 'digging', 'i', 'i\\'ve', 'oh', 'two', 'one', 'three', 'bills',\n",
    "    'number', 'ill', 'im', 'ive', 'whats', 'read', 'little', 'id', 'five', 'seven', 'four', 'six', 'eight', 'nine', 'ten',\n",
    "    'look', 'dont', 'luckily', 'get', 'lizard'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba4bb0-cf80-4f47-b8f3-f5b3c71a4406",
   "metadata": {},
   "source": [
    "## Mine Character Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91fc5c-bc40-4b12-84a5-1f22e9709df4",
   "metadata": {},
   "source": [
    "- https://medium.com/agatha-codes/using-textual-analysis-to-quantify-a-cast-of-characters-4f3baecdb5c\n",
    "- https://www.gutenberg.org/  -- Free eBook Data Source\n",
    "- https://home.aveek.io/blog/post/finding-main-characters/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f44823-b6a7-4595-838a-b240f1073cd2",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ea32f7-027b-41b4-ae65-d6033f7fa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download books data\n",
    "# nltk.download('gutenberg')\n",
    "books = [\n",
    "    nltk.corpus.gutenberg.raw('carroll-alice.txt'),\n",
    "    nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt'),\n",
    "    nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0382756-e3db-4a9b-ae04-bc553d5cf8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5c7a264-0957-4ab1-85c9-27bbfc23cc09",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba7d3d3-92b5-4bbd-82a5-3cc6364e7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(txt, punct = string.punctuation):\n",
    "    return ''.join([c for c in txt if c not in punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ce26aa-3c0d-43ee-8630-8465ea9faa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(txt, sw = list(stopwords.words('english')) + additional_sw):\n",
    "    '''\n",
    "    This function will remove the stopwords from the input txt\n",
    "    '''\n",
    "    return ' '.join([w for w in txt.split() if w.lower() not in sw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2a5c63-6c7c-4c04-8977-eb3f2d67b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    '''\n",
    "    This function will clean the text being passed by removing specific line feed characters\n",
    "    like '\\n', '\\r', and '\\'\n",
    "    '''\n",
    "    \n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('\\'', '')\n",
    "    txt = remove_punctuations(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0528e2ac-151b-4142-b3c0-b2534aa6094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 561 ms, sys: 16.2 ms, total: 577 ms\n",
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, book in enumerate(books):\n",
    "    books[i] = clean_text(book) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f907a0f9-41fc-4df1-9d93-dd1230b8b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde2bc1-4513-4d56-80e4-d9185f5eb509",
   "metadata": {},
   "source": [
    "## Identify Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b8f8af-38a9-485e-b71a-50ff79798fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(text, given_names, names):\n",
    "    '''\n",
    "    Given the list of male and female names from the NLTK library, this function will\n",
    "    parse through the input text and identify a list of names found in the input text\n",
    "    by cross referencing it with the NLTK names.\n",
    "    '''\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    for name in names:\n",
    "        if name in text and name not in given_names:\n",
    "            given_names.append(name)\n",
    "    return given_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf4099-c605-499f-9f77-bf4f3490e66a",
   "metadata": {},
   "source": [
    "### Approach 1 : Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3a8d4b-d11c-4607-9758-19ff58be555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 158\n"
     ]
    }
   ],
   "source": [
    "# source : https://stackoverflow.com/questions/55194224/extract-names-from-string-with-python-regex\n",
    "# sol 1 \n",
    "TITLE = r\"(?:[A-Z][a-z]*\\.\\s*)?\"\n",
    "NAME1 = r\"[A-Z][a-z]+,?\\s+\"\n",
    "MIDDLE_I = r\"(?:[A-Z][a-z]*\\.?\\s*)?\"\n",
    "NAME2 = r\"[A-Z][a-z]+\"\n",
    "\n",
    "sol1 = list(set(re.findall(TITLE + NAME1 + MIDDLE_I + NAME2, book)))\n",
    "\n",
    "# sol 2 \n",
    "regex = re.compile(r'([A-Z][a-z]+(?: [A-Z][a-z]\\.)? [A-Z][a-z]+)')\n",
    "sol2 = list(set(regex.findall(book)))\n",
    "\n",
    "print(len(sol1), len(sol2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea2c41-ae4c-4b5e-b13e-4cb43af0e9fb",
   "metadata": {},
   "source": [
    "### Approach 2 : NLTK Names Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fcc91be-9689-443d-8dd8-19af89d375a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of male names:\n",
      "2943\n",
      "\n",
      "Number of female names:\n",
      "5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /Users/vatsalpatel/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "nltk.download('names')\n",
    "print(\"\\nNumber of male names:\")\n",
    "print (len(names.words('male.txt')))\n",
    "print(\"\\nNumber of female names:\")\n",
    "print (len(names.words('female.txt')))\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda8039b-0dba-445b-a2d4-62d535d45875",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = [x.lower() for x in male_names]\n",
    "female_names = [x.lower() for x in female_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a64b82a-252e-47b1-a187-f5cecca01506",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_names = find_names(book, given_names = ['Alice'], names = male_names + female_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fedf0-f0df-4fdf-9d2e-5701782ea0d7",
   "metadata": {},
   "source": [
    "### Approach 3 : Government Names List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce03efc-17b5-4f77-a083-506c5bab133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# male names source : https://data.ontario.ca/dataset/ontario-top-baby-names-male\n",
    "# female names source : https://data.ontario.ca/dataset/ontario-top-baby-names-female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f98678-0a5e-43ef-ad0a-45d7d3491d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# male_names = pd.read_csv(\n",
    "#     'https://raw.githubusercontent.com/SMG-Digital/tads_age_model/Master/data/csv/ontario_top_baby_names_male_1917-2019_en_fr.csv?token=AUH4CTU4EBSNMV2GPMM4PODB3FC4G'\n",
    "# )\n",
    "\n",
    "# female_names = pd.read_csv(\n",
    "#     'https://raw.githubusercontent.com/SMG-Digital/tads_age_model/Master/data/csv/ontario_top_baby_names_female_1917-2019_en_fr.csv?token=AUH4CTRCD7TDGPSVDUB7JKLB3FDBW'\n",
    "# )\n",
    "\n",
    "# names_df = pd.concat([male_names, female_names])\n",
    "# names_df['name'] = names_df['name'].apply(lambda x : str(x).lower())\n",
    "# drop_names = [\n",
    "#     'none', 'real', 'baby', 'new', 'rock', 'harm', 'ever'\n",
    "# ]\n",
    "# remove = names_df[names_df['name'].isin(drop_names)].index\n",
    "# names_df[~names_df.index.isin(remove)].to_csv('./data/ontario_names_list_1917-2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ffe3865-9eb0-4737-bc39-d480ccb65d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.read_csv(\n",
    "    names_df_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5093b792-14a0-4b90-a779-2144956bbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = list(names_df['name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914d19c1-e196-4521-b6d9-3a44cf441e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_names = find_names(book, given_names = ['Alice'], names = names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242f4f0d-cb2d-4a90-bc64-0c514da99954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'grant', 'mark', 'royal', 'bill', 'miles', 'boy', 'drew', 'shant', 'chance', 'hung', 'sang', 'man', 'justice', 'raven', 'sage', 'deep', 'tie', 'sky', 'king', 'young', 'case', 'grey', 'bear', 'shepherd', 'may', 'rose', 'hope', 'gay', 'else', 'line', 'girl', 'velvet', 'summer', 'patience', 'tea', 'winter', 'journey', 'pepper', 'lark']\n"
     ]
    }
   ],
   "source": [
    "print(found_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f01a23-a5ec-46b5-9711-c9b4f4e3caa4",
   "metadata": {},
   "source": [
    "Other sources of mining character names were also explored like using NER, the reason I didn't go forward in showing this was because only the high end solutions yielded any useful results. Through small scale solutions like using the built in NER models in NLTK and Spacy, we ended up with too many false positives, although the results above will also yield false positives, they won't be of the magnitude of the NER solutions. The one solution through NER which did yield useful results was from training BERT on labelled named data on this corpus of text. However, although this yielded good accuracy, it was also very expensive in both time and money. I didn't have the time or resources to label thousands of books on name data and retrain the BERT model, nor did I have lot's of money to allocate towards GPUs to aid the training process. Thus the simple approach of finding names was the one I show cased right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9ceb5-122d-4222-8af9-614c086da4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ef029-62ac-4d72-a3e5-337260a98183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1e817-545e-42b9-8688-75cbaa541adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efdce14-e3d0-457c-940e-caa04483651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c7b2c-1652-4805-a279-f4ebea4d7b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60ab6779-3d8e-4719-a36b-a4839ed729d5",
   "metadata": {},
   "source": [
    "## Mining Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dc57d-2726-4bda-8d46-8bbfc604833c",
   "metadata": {},
   "source": [
    "```python\n",
    "def main():\n",
    "    filename = './twilightEdgesNames.csv'\n",
    "    savename = './twilightEdgesIDs.txt'\n",
    "    savename2 = './twilightEdgesIDsWeights.txt'\n",
    "    \n",
    "    E = pd.read_csv(filename)\n",
    "    E1 = E['Source']\n",
    "    E2 = E['Target']\n",
    "    namesText = np.unique(np.vstack((E1,E2)))\n",
    "    namesInds = [i for i in range(len(namesText))]\n",
    "    # print namesText,namesInds\n",
    "    E1 = E1.replace(namesText,namesInds)\n",
    "    E2 = E2.replace(namesText,namesInds)\n",
    "    #write to file\n",
    "    out = np.column_stack((E1,E2))\n",
    "    # labelNames = 'Source,Target'\n",
    "    np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "    #save weights too\n",
    "    np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    print \"n: %d\" % len(namesText)\n",
    "    # print \"E: %d\" % E['weight'].sum()\n",
    "    print \"E: %d\" % E.shape[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a55b3-5adc-4fef-9317-d0eaa839d49b",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bada2521-ecca-4bf7-82c0-8fd7473cdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "twilight_names = [\n",
    "    'Bella Swan', 'Edward Cullen', 'Jacob Black', 'Carlisle Cullen', 'Esme Cullen', 'Alice Cullen', 'Emmett Cullen',\n",
    "    'Rosalie Hale', 'Jasper Hale', 'Renesmee Cullen', 'James', 'Victoria', 'Laurent', 'Riley Biers', 'Bree Tanner',\n",
    "    'Sam Uley', 'Quil Ateara V', 'Embry Call', 'Paul Lahote', 'Jared Cameron', 'Leah Clearwater', 'Seth Clearwater',\n",
    "    'Collin Littlesea', 'Brady Fuller', 'Charlie Swan', 'Ren√©e Dwyer', 'Harry Clearwater', 'Billy Black', 'Tyler Crowley',\n",
    "    'Lauren Mallory', 'Mike Newton', 'Jessica Stanley', 'Angela Weber', 'Eric Yorkie', 'Emily Young', 'Sue Clearwater',\n",
    "    'Quil Ateara III', 'J. Jenks'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c127ac5-ddee-4e55-903b-936b4031c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n, char_list = twilight_names):\n",
    "    '''\n",
    "    This function will generate random data\n",
    "    '''\n",
    "    d = pd.DataFrame(\n",
    "        {\n",
    "            'Source' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Target' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Weight' : [random.randint(1, 25) for _ in range(n)]\n",
    "        }\n",
    "    ).drop_duplicates()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c07cdf0-dff9-43f0-8ab2-d616f5a90bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = sample_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7384968-e361-4c60-b4e7-acd49f8ccb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = E['Source']\n",
    "E2 = E['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a6064e-4b02-4f42-8412-154760d01c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "namesText = np.unique(np.vstack((E1,E2)))\n",
    "namesInds = [i for i in range(len(namesText))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae8818a-8210-4626-b6fe-66c1147f054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print namesText,namesInds\n",
    "E1 = E1.replace(namesText,namesInds)\n",
    "E2 = E2.replace(namesText,namesInds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "427dabf5-bd03-4c63-b6ef-4147032ecb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to file\n",
    "out = np.column_stack((E1,E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bbf9d15-e7dc-40aa-b160-4764fbd05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelNames = 'Source,Target'\n",
    "np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461c7ebd-3091-4cb4-95c5-73333742d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights too\n",
    "np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728c1b8-6ca7-49c8-beef-c0a17f048196",
   "metadata": {},
   "source": [
    "## k-Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad5166a-7094-4d59-b49a-a3006d769e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from snap import GenPrefAttach,SaveEdgeList,TRnd\n",
    "import subprocess\n",
    "\n",
    "def getDegreeList(A):\n",
    "    # n = np.unique(np.vstack((A[:,0],A[:,1]))).shape[0]\n",
    "    n = int(np.max(np.vstack((A[:,0],A[:,1]))) + 1)\n",
    "    degreeVec = np.zeros(n,dtype=int)\n",
    "    for e in range(A.shape[0]):\n",
    "        degreeVec[int(A[e,0])] += 1\n",
    "        degreeVec[int(A[e,1])] += 1\n",
    "    return degreeVec\n",
    "\n",
    "def makeWeightedEdgelist(A,outname):\n",
    "    #still remove self loops, as they make no sense in this context\n",
    "    Atmp = np.array([row for row in A if row[0] != row[1]])\n",
    "    inds = np.lexsort((Atmp[:,1],Atmp[:,0]))\n",
    "    Asort = Atmp[inds,:]\n",
    "    #get number of unique entries by taking diff\n",
    "    Adiff1 = np.vstack((np.array([1,1]),np.diff(Asort,axis=0)))\n",
    "    Adiff = np.any(Adiff1!=0,axis=1)\n",
    "    #find where the diffs are equal to 1 a and diff that to get counts of unique\n",
    "    outUnique = Asort[Adiff==1]\n",
    "    outCounts = np.diff(np.hstack((np.where(Adiff==1)[0],Adiff.shape[0])))\n",
    "    out = np.column_stack((outUnique,outCounts))\n",
    "    if outname:\n",
    "        np.savetxt(outname,out,fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "    return out\n",
    "\n",
    "\n",
    "def removeDuplicateEdges(X):\n",
    "    #remove duplicates and self loops (and also sort)\n",
    "    # xtmp = np.vstack({tuple(row) for row in X})\n",
    "    xtmp = np.vstack({tuple(row) for row in X if row[0] != row[1]})\n",
    "    inds = np.lexsort((xtmp[:,1],xtmp[:,0]))\n",
    "    out = xtmp[inds,:]\n",
    "    return out\n",
    "        \n",
    "\n",
    "def myPA(nodes,m,seed=4639):\n",
    "    np.random.seed(seed)\n",
    "    edgeList = []\n",
    "    degreeVec = np.zeros(nodes)\n",
    "    #initialize first step\n",
    "    degreeVec[0:2] = np.array([1, 1])\n",
    "    edgeList.append((0,1))\n",
    "    for n in np.arange(2,nodes):\n",
    "        #connect to existing vertices according to preferential attachment model\n",
    "        # weighting of distribution is degreeVec[:n]\n",
    "        probs = np.double(degreeVec[:n])\n",
    "        neighbors = np.random.choice(np.arange(n),m,replace=True,p=probs/np.sum(probs))\n",
    "        # print neighbors\n",
    "        degreeVec[n] = m\n",
    "        for dit in np.arange(m):\n",
    "            #if edge included, increment both degrees and append edge to the list\n",
    "            degreeVec[neighbors[dit]] += 1\n",
    "            edgeList.append((neighbors[dit],n))\n",
    "        # print degreeVec\n",
    "        # print \"avg degree: \" + str(np.sum(degreeVec)/n)\n",
    "    return np.asarray(edgeList)\n",
    "\n",
    "\n",
    "def generateGraphs(params):\n",
    "    graphname = params['graph']\n",
    "    n = int(params['n'])\n",
    "    numit = int(params['numGen'])\n",
    "    graphType = params['type']\n",
    "        \n",
    "    if graphType == 'GNP':\n",
    "        deg = int(params['d'])\n",
    "        #every node has average degree deg, total number of edges is deg*n/2, divide by total possible edges 2/(n*(n-1))\n",
    "        p = float(deg)/(n-1)\n",
    "        # print \"degree is \" + str(p)\n",
    "        np.random.seed(4639)\n",
    "        #generate all randomness at once\n",
    "        pairs = np.array([t for t in combinations(np.arange(n),2)])\n",
    "        ps = np.random.rand(pairs.shape[0],numit) <= p\n",
    "        for it in np.arange(numit):\n",
    "            #keep the edges that are sampled\n",
    "            pairsKeep = pairs[ps[:,it]==1]\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'PA':\n",
    "        deg = int(params['d'])\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            # x = myPA(n, int(deg-2./n), seed=it*4639+5011)\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            # x = myPA(n, int(deg/2.), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            # outname = graphname + '_' + graphType + 'mult_' + str(it) + '.txt'\n",
    "            # makeWeightedEdgelist(x,tmpname)\n",
    "            # np.savetxt(tmpname,x,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            xfinal = removeDuplicateEdges(x)\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            #make a weighted graph, keep track of weights for direct comparison with twilightEdgesIDsWeights.txt\n",
    "            \n",
    "    #keep the top edges that correspond to target |E| in original graph\n",
    "    elif graphType == 'Pthresh':\n",
    "        deg = int(params['d'])\n",
    "        # Etarget = deg*n/2\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            #take the Etarget edges with largest weight\n",
    "            Etarget = min(np.floor(deg*n/2.),xweighted.shape[0])\n",
    "            eind = np.argsort(xweighted[:,2])[::-1] #sort by weight\n",
    "            xtop = removeDuplicateEdges(xweighted[eind[:Etarget],:2])\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            \n",
    "\n",
    "    elif graphType == 'PAsnap':\n",
    "        deg = int(params['d'])\n",
    "        Trnd1 = TRnd()\n",
    "        for it in np.arange(numit):\n",
    "            #generate graph\n",
    "            Trnd1.PutSeed(it*4639+5011)\n",
    "            x = GenPrefAttach(n,deg,Trnd1)\n",
    "            #save output\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            SaveEdgeList(x,outname)\n",
    "            #remove the top 3 lines, sed -i '' -e 1,3d tmp.txt\n",
    "            emp = ''\n",
    "            out = subprocess.call([\"sed\", \"-i\", emp, \"-e\", \"1,3d\", outname])\n",
    "            \n",
    "    elif graphType == 'CL':\n",
    "        #get degree sequence from input\n",
    "        w = params['dList']\n",
    "        wnorm = float(np.sum(w))\n",
    "        nc2 = int(n*(n-1)/2)\n",
    "        pairs = np.zeros((nc2,2))\n",
    "        pairComp = np.zeros(nc2)\n",
    "        for e,(i,j) in enumerate(combinations(np.arange(n),2)):\n",
    "            #array comparison\n",
    "            pairComp[e] = w[i]*w[j]/wnorm\n",
    "            pairs[e,0] = i\n",
    "            pairs[e,1] = j\n",
    "        rands = np.random.rand(nc2,numit)\n",
    "        for it in np.arange(numit):\n",
    "                pairsKeep = pairs[rands[:,it] < pairComp]\n",
    "                outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "                np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'CNFG':\n",
    "        w = params['dList']\n",
    "        wnorm = int(np.sum(w))\n",
    "        elist = np.zeros(wnorm)\n",
    "        st = 0\n",
    "        for i,wi in enumerate(w):\n",
    "            elist[st:(st+wi)] = i\n",
    "            st += wi\n",
    "        for it in np.arange(numit):\n",
    "            plist = np.random.permutation(elist)\n",
    "            x = plist.reshape(-1,2)\n",
    "            #if column 1 is greater than column 0 then swap that column\n",
    "            xswap = x[:,0] > x[:,1]\n",
    "            x[xswap,0:2] = np.column_stack((x[xswap,1],x[xswap,0]))\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_wt.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            #sort correctly and remove self loops, duplicates\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            np.savetxt(outname,xweighted[:,:2],fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    #example parameters\n",
    "    #123 undirected, but 1031 total weight if including multiedges\n",
    "#     params = {'graph': 'twilight','type':'PA','n': 27,'d': int(2*1031/27),'numGen': 3}\n",
    "    params = {'graph': 'twilight','type':'CL','n': 3,'dList': [int(2*1031/27), 72, 76],'numGen': 3}\n",
    "\n",
    "    # params = {'graph': 'twilight','type':'PA','n': 27,'d': int(2*123/27),'numGen': 3}\n",
    "    # 575 undirected, but 9464 total weight if including multiedges\n",
    "#     params = {'graph': 'goblet','type':'PA','n': 62,'d': int(2*575/62),'numGen': 3}\n",
    "    generateGraphs(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbbd19-cd4f-47f1-a58b-d701694b5742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61746c3c-ae2b-4247-8745-7abe8c2151d1",
   "metadata": {},
   "source": [
    "# Modelling Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774da8d3-23b2-49b5-8f76-ce6b0b9a3313",
   "metadata": {},
   "source": [
    "## Preferential Attachement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53e6d05b-1a11-4ac3-9133-12e579e1c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from snap import GenPrefAttach,SaveEdgeList,TRnd\n",
    "from sklearn import svm, base, feature_selection, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve,auc,classification_report,f1_score,accuracy_score,roc_auc_score\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import subprocess\n",
    "# from generateGraphsMain import *\n",
    "from os import listdir\n",
    "\n",
    "#to visualize\n",
    "from six import StringIO\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc111d5d-270d-4d97-b62a-a893ed16a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random graphs...\n",
      "initializing directory and taking features of original graph...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['mkdir', 'graphs']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-34f43a7efe36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m#automate initializing directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m#this will throw an error if there is already a folder named graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0minitializeDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-34f43a7efe36>\u001b[0m in \u001b[0;36minitializeDirectory\u001b[0;34m(origGraph)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m#compute graph profile features using GraphLab PowerGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initializing directory and taking features of original graph...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mkdir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graphs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#graph\\tsample_prob_keep\\tn3_3\\tn3_2\\tn3_1\\tn3_0\\tn4_0\\tn4_1\\tn4_2\\tn4_3\\tn4_4\\tn4_5\\tn4_6\\tn4_7\\tn4_8\\tn4_9\\tn4_10\\truntime\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'counts_4_profilesLocal.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    416\u001b[0m                **kwargs).stdout\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['mkdir', 'graphs']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "def doSVMcvPrediction(Xin,yin,Xtest,ytest,modType):\n",
    "    #input: training/test data and labels, model type\n",
    "    #supported models: SVM (l1 and l2), AdaBoost, decision tree, and random forest\n",
    "    #train with 5-fold cross validation, then test once using test (holdout) data\n",
    "    #once the best estimator is chosen here, train on the entire dataset (in + test) outside this function\n",
    "    #output: training accuracy, gereralization accuracy, feature weights/importances, classifier, \n",
    "    #  classification report, training f1-score and generalization f1-score\n",
    "    nfolds = 5\n",
    "    cv = cross_val_score(yin,nfolds,shuffle=True)\n",
    "    #l1 penalty enforces sparsity in weights, only available for linear SVM classifier\n",
    "    if modType in ('SVM-L2','svm-l2'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l2', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            # scoring='roc_auc',\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('SVM-L1','svm-l1'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l1', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "    \n",
    "    #decision tree classifiers\n",
    "    elif modType in ('ada','adaboost','adaboost-tree'):\n",
    "        clasf = AdaBoostClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'learning_rate' : [0.1,0.3,0.5]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('dtree','decision-tree'):\n",
    "        clasf = DecisionTreeClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'splitter' : ['best'],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('rf','random-forest'):\n",
    "        clasf = RandomForestClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "        \n",
    "    #TODO: add linear regression, logistic regression, etc. \n",
    "\n",
    "    cvclasf.fit(Xin,yin)\n",
    "    bclasf = cvclasf.best_estimator_\n",
    "    print(\"%s %d-fold CV params: %s\" % (modType,nfolds,cvclasf.best_params_))\n",
    "    \n",
    "    if modType in ('ada','adaboost-tree','dtree','decision-tree','rf','random-forest'):\n",
    "        w = bclasf.feature_importances_\n",
    "    elif modType in ('SVM-L1','svm-l1','SVM-L2','svm-l2'):\n",
    "        w = bclasf.coef_\n",
    "    \n",
    "    bclasf.fit(Xin,yin)\n",
    "    y_train_pred = bclasf.predict(Xin)\n",
    "    acTrain = accuracy_score(yin,y_train_pred)\n",
    "    f1Train = f1_score(yin,y_train_pred,average=\"weighted\")\n",
    "    \n",
    "    y_pred = bclasf.predict(Xtest)\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    acGeneral = accuracy_score(ytest, y_pred)\n",
    "    f1Gen = f1_score(ytest,y_pred,average=\"weighted\")\n",
    "\n",
    "    return(acTrain,np.squeeze(w),bclasf,report,(acTrain,acGeneral),(f1Train,f1Gen))\n",
    "\n",
    "def initializeDirectory(origGraph):\n",
    "    #compute graph profile features using GraphLab PowerGraph\n",
    "    print(\"initializing directory and taking features of original graph...\")\n",
    "    out = subprocess.check_output([\"mkdir\", \"graphs\"])\n",
    "    hdr = \"#graph\\tsample_prob_keep\\tn3_3\\tn3_2\\tn3_1\\tn3_0\\tn4_0\\tn4_1\\tn4_2\\tn4_3\\tn4_4\\tn4_5\\tn4_6\\tn4_7\\tn4_8\\tn4_9\\tn4_10\\truntime\\n\"\n",
    "    with open('counts_4_profilesLocal.txt', 'w') as fpt:\n",
    "        fpt.write(hdr)\n",
    "    pcommand = '/Users/vatsalpatel'\n",
    "    out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", origGraph])\n",
    "    hdr2 = \"#graph\\tevbin0\\tevbin1\\tevbin2\\tevbin3\\tevbin4\\n\"\n",
    "    with open('counts_eval_bins.txt', 'w') as fpt:\n",
    "        fpt.write(hdr2)\n",
    "    generateEigenvalueBins(origGraph,\"counts_eval_bins.txt\")\n",
    "    return 0\n",
    "\n",
    "def generateEigenvalueBins(gname,outDir,nbins=5):\n",
    "    #get normalized laplacian\n",
    "    hbins = np.histogram(np.array([0,2]),bins=nbins)[1]\n",
    "    E = np.loadtxt(gname,delimiter='\\t')\n",
    "    #map everything to number of unique vertices\n",
    "    un = np.unique(np.vstack((E[:,0],E[:,1])))\n",
    "    n = len(un)\n",
    "    A = np.zeros((n,n))\n",
    "    for e in np.arange(E.shape[0]):\n",
    "        tmp0 = np.argwhere(un==E[e,0])\n",
    "        tmp1 = np.argwhere(un==E[e,1])\n",
    "        A[tmp0,tmp1] = 1\n",
    "        A[tmp1,tmp0] = 1\n",
    "    D = np.diag(np.sum(A,1))\n",
    "    Di = np.linalg.inv(np.sqrt(D))\n",
    "    L = np.eye(n) - Di.dot(A).dot(Di)\n",
    "    teigs = np.linalg.eigvalsh(L)\n",
    "    #take histogram\n",
    "    neig = len(teigs.flatten()) + nbins\n",
    "    nh = np.histogram(teigs,bins=hbins)[0]    \n",
    "    ep = (nh + 1.)/neig #add smoothing and normalize\n",
    "    #append to file\n",
    "    with open(outDir, \"a\") as myfile:\n",
    "        myfile.write(gname + \"\\t\" +  \"\\t\".join([str(e) for e in ep]) + \"\\n\")\n",
    "    return 0\n",
    "\n",
    "def writeTree(treeModel,namesList,filename):\n",
    "    #utility function that plots a decision tree and saves to file\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(treeModel,out_file=dot_data,feature_names=namesList)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(filename) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    genData = 1 #flag to generate random graphs\n",
    "    classify = 0 #flag to classify\n",
    "    useSpectral = 0 #flag to include eigenvalue histograms\n",
    "\n",
    "    if genData:\n",
    "\n",
    "        print(\"generating random graphs...\")\n",
    "        # #generate graphs, these parameters should be easy to automate\n",
    "        # n,E,filename,outname = 27,123,'./twilightEdgesIDsWeights.txt','graphs/twilight'\n",
    "        # n,E,filename,outname = 27,1031,'./twilightEdgesIDsWeights.txt','graphs/twilight'\n",
    "        n,E,filename,outname = 39,280,'./thestandEdgesIDsWeights.txt','graphs/thestand'\n",
    "        # n,E,filename,outname = 39,6539,'./thestandEdgesIDsWeights.txt','graphs/thestand'\n",
    "        # n,E,filename,outname = 62,575,'./gobletEdgesIDsWeights.txt','graphs/goblet'\n",
    "        # n,E,filename,outname = 62,9464,'./gobletEdgesIDsWeights.txt','graphs/goblet'\n",
    "        \n",
    "        #automate initializing directory\n",
    "        #this will throw an error if there is already a folder named graphs\n",
    "        initializeDirectory(filename)\n",
    "\n",
    "        A = np.loadtxt(filename,delimiter='\\t')\n",
    "        degreeVec = getDegreeList(A)\n",
    "\n",
    "        params = {'graph': outname,'type':'CNFG','n': n,'dList': degreeVec,'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        params = {'graph': outname,'type':'CL','n': n,'dList': degreeVec,'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        \n",
    "        # params = {'graph': outname,'type':'PA','n': n,'d': int(2*E/n),'numGen': 50}\n",
    "        params = {'graph': outname,'type':'PA','n': n,'d': int(2*E/n),'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        params = {'graph': outname,'type':'GNP','n': n,'d': int(2*E/n),'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        # #generate 100 graphs each from 3 classes in < 3 seconds\n",
    "        #thresholded PA model\n",
    "       \n",
    "\n",
    "        graph_dir = '/Users/vatsalpatel/Documents/novels/graphs/'\n",
    "        #feature design 1\n",
    "        #analyze global 4-profiles\n",
    "        #separate by generative model?\n",
    "        # /Users/ethan/graphlab-master-2/release/apps/4-profiles/4profile --format tsv --graph /Users/ethan/Documents/novels/twilightEdgesIDs.txt\n",
    "        print(\"computing global subgraph counts...\")\n",
    "        pcommand = '/Users/vatsalpatel'\n",
    "        for gname in listdir(graph_dir):\n",
    "            if gname.endswith('.txt') and \"_wt\" not in gname:\n",
    "            # if \"_CL_\" in gname:\n",
    "                # out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", gname, \"--per_vertex\", gname])\n",
    "                out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", graph_dir + gname])\n",
    "                if useSpectral:\n",
    "                    generateEigenvalueBins(graph_dir+gname,\"counts_eval_bins.txt\")\n",
    "        out = subprocess.check_output([\"mv\", \"counts_4_profilesLocal.txt\", \"graphs\"])\n",
    "        out = subprocess.check_output([\"mv\", \"counts_eval_bins.txt\", \"graphs\"])\n",
    "        #global 4 profiles for 100 graphs each from 3 classes in ~1 minute\n",
    "\n",
    "        # additional feature design could include distributions of local 4-profiles throughout graph\n",
    "        # or pagerank or centrality measures\n",
    "        \n",
    "    if classify:\n",
    "        #build classifiers\n",
    "        #as a baseline, split data into train and test \n",
    "        #this will verify the classifier can differentiate between graph families\n",
    "        \n",
    "        np.random.seed(423322) #for repeatability during writeup\n",
    "        graphFolder = 'graphsGoblet/'\n",
    "        # graphFolder = 'graphsTwilight/'\n",
    "        # graphFolder = 'graphsTheStand/'\n",
    "        print(\"Reading data from folder %s\" % graphFolder)\n",
    "\n",
    "        #read data from 4-profile output file, read labels and split into train and test\n",
    "        featInds = np.arange(2,17)\n",
    "        D = pd.read_csv(graphFolder + 'counts_4_profilesLocal.txt',delimiter='\\t')\n",
    "        X = np.array(D.ix[1:,featInds])\n",
    "        # print X[0,:]\n",
    "        #add eigenvalue histogram to X\n",
    "        if useSpectral:\n",
    "            #assume its in the exact same order\n",
    "            featInds2 = np.arange(1,6) #5 bins\n",
    "            D2 = pd.read_csv(graphFolder + 'counts_eval_bins.txt',delimiter='\\t')\n",
    "            X = np.hstack((X,np.array(D2.ix[1:,featInds2])))\n",
    "            # X = np.array(D2.ix[1:,featInds2])\n",
    "        # print X[0,:]\n",
    "        y = np.zeros(X.shape[0])\n",
    "        # D.ix[D['#graph'].str.contains('CL'),2:17]\n",
    "        RGfamilies = ['CL','GNP','PA','CNFG']\n",
    "        # RGfamilies = ['CL','GNP','PA','PAmult']\n",
    "        for i,s in enumerate(RGfamilies):\n",
    "        #     print(i)\n",
    "            y[np.array(D['#graph'].ix[1:].str.contains(s))] = i\n",
    "        # print y\n",
    "        holdfrac = 0.5\n",
    "        Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=holdfrac,stratify=y)\n",
    "\n",
    "        #preprocess\n",
    "        scaler1=StandardScaler()\n",
    "        Xtrain = scaler1.fit_transform(Xtrain.astype(np.double))\n",
    "        Xtest = scaler1.transform(Xtest.astype(np.double))\n",
    "        \n",
    "        # modelType = 'SVM-L2'\n",
    "        # modelType = 'SVM-L1'\n",
    "        # modelType = 'adaboost-tree'\n",
    "        modelType = 'decision-tree'\n",
    "        # modelType = 'random-forest'\n",
    "        score,optWeights,clasf,rep,accs,f1s = doSVMcvPrediction(Xtrain, ytrain, Xtest, ytest, modelType)\n",
    "        #classifier works perfectly\n",
    "        # print clasf.get_params\n",
    "        print(\"Checking distinctness of random graph families...\")\n",
    "        print(rep)\n",
    "        # print accs[0],accs[1],f1s[0],f1s[1]\n",
    "        # print np.column_stack((optWeights.T,D.columns[featInds]))\n",
    "        \n",
    "        #see which random graph model the novel gets classified as\n",
    "        x = np.array(D.ix[0,featInds]).reshape(1,-1)\n",
    "        if useSpectral:\n",
    "            x = np.hstack((x,np.array(D2.ix[0,featInds2]).reshape(1,-1)))\n",
    "            # x = np.array(D2.ix[0,featInds2]).reshape(1,-1)\n",
    "        scaler=StandardScaler()\n",
    "        X = scaler.fit_transform(X.astype(np.double))\n",
    "        x = scaler.transform(x.astype(np.double)) \n",
    "        clasf.fit(X,y)\n",
    "        novelRG = clasf.predict(x)\n",
    "        if useSpectral:\n",
    "            Fcol = D.columns[featInds].append(D2.columns[featInds2])\n",
    "            # Fcol = D2.columns[featInds2]\n",
    "        else:\n",
    "            Fcol = D.columns[featInds]\n",
    "        # print Fcol\n",
    "        if modelType in ('SVM-L1','SVM-L2'):\n",
    "            #TODO: print statements when useSpectral\n",
    "            print(\"SVM feature weights:\")\n",
    "            print(np.column_stack((clasf.coef_.T,Fcol)))\n",
    "            sc = x.dot(clasf.coef_.T) + clasf.intercept_\n",
    "            print(\"scores:\")\n",
    "            # print np.vstack((RGfamilies,(sc-np.min(sc))/np.sum(sc-np.min(sc))))\n",
    "            print(np.vstack((RGfamilies,sc)))\n",
    "        elif modelType in ('adaboost-tree'):\n",
    "            print(\"Adaboost feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            # print \"prediction probabilities:\" \n",
    "            # print np.vstack((RGfamilies,clasf.predict_proba(x)))\n",
    "            print(\"decision function:\")\n",
    "            print(np.vstack((RGfamilies,clasf.decision_function(x))))\n",
    "            #also the actual tree?\n",
    "        elif modelType in ('decision-tree'):\n",
    "            print(\"Decision tree feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            print(\"prediction probabilities:\" )\n",
    "            print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "            #also the actual tree\n",
    "            Fcol2 = [s.replace('n3','H').replace('n4','F') for s in Fcol]\n",
    "            writeTree(clasf,Fcol2,graphFolder[:-1]+'_dTree.pdf')\n",
    "        elif modelType in ('random-forest'):\n",
    "            print(\"Random Forest feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            print(\"prediction probabilities:\" )\n",
    "            print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "        #print the prediction (and confidence score?)\n",
    "        print(\"Fiction novel classified as: %s\" % RGfamilies[int(novelRG)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6661c-c528-443e-bbcb-f7a0c32c6ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626ff71-34ae-4731-94ed-a994a17ffc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c969f59f-cdcd-4abd-a919-efb3780667bd",
   "metadata": {},
   "source": [
    "## Chung-Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed6ad2-5288-4a0c-942a-b0ff174fa75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd00bfe-59e4-486c-b74d-a94aa111b414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dabb369a-9472-4dea-8a3b-c6a0b733316d",
   "metadata": {},
   "source": [
    "## Binomial Random Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37044e0-e667-4ec4-89f4-26a253bceb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239da31-75c0-4fd0-b3f8-b275ca548dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e53bcb-61ce-43a2-a180-be057aa922e5",
   "metadata": {},
   "source": [
    "## Configuration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57c92d-d365-4a01-ae7f-51307118dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a602cf-c81e-4101-a753-744be6787cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42599d25-12bb-4a27-8a67-73875b6197b0",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9223e9e-b820-4ca5-8b75-fb3194bb0324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aba390-2cc6-47ab-a3a9-f8e9a71157ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3de60b-923c-4168-88b9-36aaf2ca1454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
