{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a801d278-6b81-472b-9cbb-8fb0f8213172",
   "metadata": {},
   "source": [
    "# Mining & Modelling Character Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb354d5-34b5-41d3-acc2-29e486d12f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import io\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a421c37-8d92-46d0-b14d-de1a31a63574",
   "metadata": {},
   "source": [
    "Paper Reference [Here](https://math.ryerson.ca/~abonato/papers/CharacterNetworks_WAW_Aug1_BDAEGH.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e73969d-37cc-4079-a88a-f470c3e30c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "data_path = './data/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab6779-3d8e-4719-a36b-a4839ed729d5",
   "metadata": {},
   "source": [
    "## Mining Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dc57d-2726-4bda-8d46-8bbfc604833c",
   "metadata": {},
   "source": [
    "```python\n",
    "def main():\n",
    "    filename = './twilightEdgesNames.csv'\n",
    "    savename = './twilightEdgesIDs.txt'\n",
    "    savename2 = './twilightEdgesIDsWeights.txt'\n",
    "    \n",
    "    E = pd.read_csv(filename)\n",
    "    E1 = E['Source']\n",
    "    E2 = E['Target']\n",
    "    namesText = np.unique(np.vstack((E1,E2)))\n",
    "    namesInds = [i for i in range(len(namesText))]\n",
    "    # print namesText,namesInds\n",
    "    E1 = E1.replace(namesText,namesInds)\n",
    "    E2 = E2.replace(namesText,namesInds)\n",
    "    #write to file\n",
    "    out = np.column_stack((E1,E2))\n",
    "    # labelNames = 'Source,Target'\n",
    "    np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "    #save weights too\n",
    "    np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    print \"n: %d\" % len(namesText)\n",
    "    # print \"E: %d\" % E['weight'].sum()\n",
    "    print \"E: %d\" % E.shape[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a55b3-5adc-4fef-9317-d0eaa839d49b",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bada2521-ecca-4bf7-82c0-8fd7473cdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "twilight_names = [\n",
    "    'Bella Swan', 'Edward Cullen', 'Jacob Black', 'Carlisle Cullen', 'Esme Cullen', 'Alice Cullen', 'Emmett Cullen',\n",
    "    'Rosalie Hale', 'Jasper Hale', 'Renesmee Cullen', 'James', 'Victoria', 'Laurent', 'Riley Biers', 'Bree Tanner',\n",
    "    'Sam Uley', 'Quil Ateara V', 'Embry Call', 'Paul Lahote', 'Jared Cameron', 'Leah Clearwater', 'Seth Clearwater',\n",
    "    'Collin Littlesea', 'Brady Fuller', 'Charlie Swan', 'Ren√©e Dwyer', 'Harry Clearwater', 'Billy Black', 'Tyler Crowley',\n",
    "    'Lauren Mallory', 'Mike Newton', 'Jessica Stanley', 'Angela Weber', 'Eric Yorkie', 'Emily Young', 'Sue Clearwater',\n",
    "    'Quil Ateara III', 'J. Jenks'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c127ac5-ddee-4e55-903b-936b4031c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n, char_list = twilight_names):\n",
    "    '''\n",
    "    This function will generate random data\n",
    "    '''\n",
    "    d = pd.DataFrame(\n",
    "        {\n",
    "            'Source' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Target' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Weight' : [random.randint(1, 25) for _ in range(n)]\n",
    "        }\n",
    "    ).drop_duplicates()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c07cdf0-dff9-43f0-8ab2-d616f5a90bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = sample_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7384968-e361-4c60-b4e7-acd49f8ccb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = E['Source']\n",
    "E2 = E['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a6064e-4b02-4f42-8412-154760d01c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "namesText = np.unique(np.vstack((E1,E2)))\n",
    "namesInds = [i for i in range(len(namesText))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae8818a-8210-4626-b6fe-66c1147f054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print namesText,namesInds\n",
    "E1 = E1.replace(namesText,namesInds)\n",
    "E2 = E2.replace(namesText,namesInds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "427dabf5-bd03-4c63-b6ef-4147032ecb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to file\n",
    "out = np.column_stack((E1,E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bbf9d15-e7dc-40aa-b160-4764fbd05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelNames = 'Source,Target'\n",
    "np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461c7ebd-3091-4cb4-95c5-73333742d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights too\n",
    "np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728c1b8-6ca7-49c8-beef-c0a17f048196",
   "metadata": {},
   "source": [
    "## k-Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bad5166a-7094-4d59-b49a-a3006d769e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from snap import GenPrefAttach,SaveEdgeList,TRnd\n",
    "import subprocess\n",
    "\n",
    "def getDegreeList(A):\n",
    "    # n = np.unique(np.vstack((A[:,0],A[:,1]))).shape[0]\n",
    "    n = int(np.max(np.vstack((A[:,0],A[:,1]))) + 1)\n",
    "    degreeVec = np.zeros(n,dtype=int)\n",
    "    for e in range(A.shape[0]):\n",
    "        degreeVec[int(A[e,0])] += 1\n",
    "        degreeVec[int(A[e,1])] += 1\n",
    "    return degreeVec\n",
    "\n",
    "def makeWeightedEdgelist(A,outname):\n",
    "    #still remove self loops, as they make no sense in this context\n",
    "    Atmp = np.array([row for row in A if row[0] != row[1]])\n",
    "    inds = np.lexsort((Atmp[:,1],Atmp[:,0]))\n",
    "    Asort = Atmp[inds,:]\n",
    "    #get number of unique entries by taking diff\n",
    "    Adiff1 = np.vstack((np.array([1,1]),np.diff(Asort,axis=0)))\n",
    "    Adiff = np.any(Adiff1!=0,axis=1)\n",
    "    #find where the diffs are equal to 1 a and diff that to get counts of unique\n",
    "    outUnique = Asort[Adiff==1]\n",
    "    outCounts = np.diff(np.hstack((np.where(Adiff==1)[0],Adiff.shape[0])))\n",
    "    out = np.column_stack((outUnique,outCounts))\n",
    "    if outname:\n",
    "        np.savetxt(outname,out,fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "    return out\n",
    "\n",
    "\n",
    "def removeDuplicateEdges(X):\n",
    "    #remove duplicates and self loops (and also sort)\n",
    "    # xtmp = np.vstack({tuple(row) for row in X})\n",
    "    xtmp = np.vstack({tuple(row) for row in X if row[0] != row[1]})\n",
    "    inds = np.lexsort((xtmp[:,1],xtmp[:,0]))\n",
    "    out = xtmp[inds,:]\n",
    "    return out\n",
    "        \n",
    "\n",
    "def myPA(nodes,m,seed=4639):\n",
    "    np.random.seed(seed)\n",
    "    edgeList = []\n",
    "    degreeVec = np.zeros(nodes)\n",
    "    #initialize first step\n",
    "    degreeVec[0:2] = np.array([1, 1])\n",
    "    edgeList.append((0,1))\n",
    "    for n in np.arange(2,nodes):\n",
    "        #connect to existing vertices according to preferential attachment model\n",
    "        # weighting of distribution is degreeVec[:n]\n",
    "        probs = np.double(degreeVec[:n])\n",
    "        neighbors = np.random.choice(np.arange(n),m,replace=True,p=probs/np.sum(probs))\n",
    "        # print neighbors\n",
    "        degreeVec[n] = m\n",
    "        for dit in np.arange(m):\n",
    "            #if edge included, increment both degrees and append edge to the list\n",
    "            degreeVec[neighbors[dit]] += 1\n",
    "            edgeList.append((neighbors[dit],n))\n",
    "        # print degreeVec\n",
    "        # print \"avg degree: \" + str(np.sum(degreeVec)/n)\n",
    "    return np.asarray(edgeList)\n",
    "\n",
    "\n",
    "def generateGraphs(params):\n",
    "    graphname = params['graph']\n",
    "    n = int(params['n'])\n",
    "    numit = int(params['numGen'])\n",
    "    graphType = params['type']\n",
    "        \n",
    "    if graphType == 'GNP':\n",
    "        deg = int(params['d'])\n",
    "        #every node has average degree deg, total number of edges is deg*n/2, divide by total possible edges 2/(n*(n-1))\n",
    "        p = float(deg)/(n-1)\n",
    "        # print \"degree is \" + str(p)\n",
    "        np.random.seed(4639)\n",
    "        #generate all randomness at once\n",
    "        pairs = np.array([t for t in combinations(np.arange(n),2)])\n",
    "        ps = np.random.rand(pairs.shape[0],numit) <= p\n",
    "        for it in np.arange(numit):\n",
    "            #keep the edges that are sampled\n",
    "            pairsKeep = pairs[ps[:,it]==1]\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'PA':\n",
    "        deg = int(params['d'])\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            # x = myPA(n, int(deg-2./n), seed=it*4639+5011)\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            # x = myPA(n, int(deg/2.), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            # outname = graphname + '_' + graphType + 'mult_' + str(it) + '.txt'\n",
    "            # makeWeightedEdgelist(x,tmpname)\n",
    "            # np.savetxt(tmpname,x,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            xfinal = removeDuplicateEdges(x)\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            #make a weighted graph, keep track of weights for direct comparison with twilightEdgesIDsWeights.txt\n",
    "            \n",
    "    #keep the top edges that correspond to target |E| in original graph\n",
    "    elif graphType == 'Pthresh':\n",
    "        deg = int(params['d'])\n",
    "        # Etarget = deg*n/2\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            #take the Etarget edges with largest weight\n",
    "            Etarget = min(np.floor(deg*n/2.),xweighted.shape[0])\n",
    "            eind = np.argsort(xweighted[:,2])[::-1] #sort by weight\n",
    "            xtop = removeDuplicateEdges(xweighted[eind[:Etarget],:2])\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            \n",
    "\n",
    "    elif graphType == 'PAsnap':\n",
    "        deg = int(params['d'])\n",
    "        Trnd1 = TRnd()\n",
    "        for it in np.arange(numit):\n",
    "            #generate graph\n",
    "            Trnd1.PutSeed(it*4639+5011)\n",
    "            x = GenPrefAttach(n,deg,Trnd1)\n",
    "            #save output\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            SaveEdgeList(x,outname)\n",
    "            #remove the top 3 lines, sed -i '' -e 1,3d tmp.txt\n",
    "            emp = ''\n",
    "            out = subprocess.call([\"sed\", \"-i\", emp, \"-e\", \"1,3d\", outname])\n",
    "            \n",
    "    elif graphType == 'CL':\n",
    "        #get degree sequence from input\n",
    "        w = params['dList']\n",
    "        wnorm = float(np.sum(w))\n",
    "        nc2 = n*(n-1)/2\n",
    "        pairs = np.zeros((nc2,2))\n",
    "        pairComp = np.zeros(nc2)\n",
    "        for e,(i,j) in enumerate(combinations(np.arange(n),2)):\n",
    "            #array comparison\n",
    "            pairComp[e] = w[i]*w[j]/wnorm\n",
    "            pairs[e,0] = i\n",
    "            pairs[e,1] = j\n",
    "        rands = np.random.rand(nc2,numit)\n",
    "        for it in np.arange(numit):\n",
    "                pairsKeep = pairs[rands[:,it] < pairComp]\n",
    "                outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "                np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'CNFG':\n",
    "        w = params['dList']\n",
    "        wnorm = np.sum(w)\n",
    "        elist = np.zeros(wnorm)\n",
    "        st = 0\n",
    "        for i,wi in enumerate(w):\n",
    "            elist[st:(st+wi)] = i\n",
    "            st += wi\n",
    "        for it in np.arange(numit):\n",
    "            plist = np.random.permutation(elist)\n",
    "            x = plist.reshape(-1,2)\n",
    "            #if column 1 is greater than column 0 then swap that column\n",
    "            xswap = x[:,0] > x[:,1]\n",
    "            x[xswap,0:2] = np.column_stack((x[xswap,1],x[xswap,0]))\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_wt.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            #sort correctly and remove self loops, duplicates\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            np.savetxt(outname,xweighted[:,:2],fmt=('%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c5a4a-a59c-4261-8428-4552bdd2258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103308b8-39c2-45ea-acb7-6f73601f4174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbbd19-cd4f-47f1-a58b-d701694b5742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61746c3c-ae2b-4247-8745-7abe8c2151d1",
   "metadata": {},
   "source": [
    "# Modelling Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774da8d3-23b2-49b5-8f76-ce6b0b9a3313",
   "metadata": {},
   "source": [
    "## Preferential Attachement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc111d5d-270d-4d97-b62a-a893ed16a074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c969f59f-cdcd-4abd-a919-efb3780667bd",
   "metadata": {},
   "source": [
    "## Chung-Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed6ad2-5288-4a0c-942a-b0ff174fa75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd00bfe-59e4-486c-b74d-a94aa111b414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dabb369a-9472-4dea-8a3b-c6a0b733316d",
   "metadata": {},
   "source": [
    "## Binomial Random Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37044e0-e667-4ec4-89f4-26a253bceb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239da31-75c0-4fd0-b3f8-b275ca548dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e53bcb-61ce-43a2-a180-be057aa922e5",
   "metadata": {},
   "source": [
    "## Configuration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57c92d-d365-4a01-ae7f-51307118dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a602cf-c81e-4101-a753-744be6787cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42599d25-12bb-4a27-8a67-73875b6197b0",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9223e9e-b820-4ca5-8b75-fb3194bb0324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aba390-2cc6-47ab-a3a9-f8e9a71157ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3de60b-923c-4168-88b9-36aaf2ca1454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
