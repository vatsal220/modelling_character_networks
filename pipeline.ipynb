{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a801d278-6b81-472b-9cbb-8fb0f8213172",
   "metadata": {},
   "source": [
    "# Mining & Modelling Character Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb354d5-34b5-41d3-acc2-29e486d12f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import io\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a421c37-8d92-46d0-b14d-de1a31a63574",
   "metadata": {},
   "source": [
    "Paper Reference [Here](https://math.ryerson.ca/~abonato/papers/CharacterNetworks_WAW_Aug1_BDAEGH.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e73969d-37cc-4079-a88a-f470c3e30c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "data_path = './data/data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab6779-3d8e-4719-a36b-a4839ed729d5",
   "metadata": {},
   "source": [
    "## Mining Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dc57d-2726-4bda-8d46-8bbfc604833c",
   "metadata": {},
   "source": [
    "```python\n",
    "def main():\n",
    "    filename = './twilightEdgesNames.csv'\n",
    "    savename = './twilightEdgesIDs.txt'\n",
    "    savename2 = './twilightEdgesIDsWeights.txt'\n",
    "    \n",
    "    E = pd.read_csv(filename)\n",
    "    E1 = E['Source']\n",
    "    E2 = E['Target']\n",
    "    namesText = np.unique(np.vstack((E1,E2)))\n",
    "    namesInds = [i for i in range(len(namesText))]\n",
    "    # print namesText,namesInds\n",
    "    E1 = E1.replace(namesText,namesInds)\n",
    "    E2 = E2.replace(namesText,namesInds)\n",
    "    #write to file\n",
    "    out = np.column_stack((E1,E2))\n",
    "    # labelNames = 'Source,Target'\n",
    "    np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "    #save weights too\n",
    "    np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    print \"n: %d\" % len(namesText)\n",
    "    # print \"E: %d\" % E['weight'].sum()\n",
    "    print \"E: %d\" % E.shape[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a55b3-5adc-4fef-9317-d0eaa839d49b",
   "metadata": {},
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bada2521-ecca-4bf7-82c0-8fd7473cdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "twilight_names = [\n",
    "    'Bella Swan', 'Edward Cullen', 'Jacob Black', 'Carlisle Cullen', 'Esme Cullen', 'Alice Cullen', 'Emmett Cullen',\n",
    "    'Rosalie Hale', 'Jasper Hale', 'Renesmee Cullen', 'James', 'Victoria', 'Laurent', 'Riley Biers', 'Bree Tanner',\n",
    "    'Sam Uley', 'Quil Ateara V', 'Embry Call', 'Paul Lahote', 'Jared Cameron', 'Leah Clearwater', 'Seth Clearwater',\n",
    "    'Collin Littlesea', 'Brady Fuller', 'Charlie Swan', 'Ren√©e Dwyer', 'Harry Clearwater', 'Billy Black', 'Tyler Crowley',\n",
    "    'Lauren Mallory', 'Mike Newton', 'Jessica Stanley', 'Angela Weber', 'Eric Yorkie', 'Emily Young', 'Sue Clearwater',\n",
    "    'Quil Ateara III', 'J. Jenks'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c127ac5-ddee-4e55-903b-936b4031c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(n, char_list = twilight_names):\n",
    "    '''\n",
    "    This function will generate random data\n",
    "    '''\n",
    "    d = pd.DataFrame(\n",
    "        {\n",
    "            'Source' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Target' : [random.choice(twilight_names) for _ in range(n)],\n",
    "            'Weight' : [random.randint(1, 25) for _ in range(n)]\n",
    "        }\n",
    "    ).drop_duplicates()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c07cdf0-dff9-43f0-8ab2-d616f5a90bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = sample_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7384968-e361-4c60-b4e7-acd49f8ccb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E1 = E['Source']\n",
    "E2 = E['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a6064e-4b02-4f42-8412-154760d01c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "namesText = np.unique(np.vstack((E1,E2)))\n",
    "namesInds = [i for i in range(len(namesText))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae8818a-8210-4626-b6fe-66c1147f054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print namesText,namesInds\n",
    "E1 = E1.replace(namesText,namesInds)\n",
    "E2 = E2.replace(namesText,namesInds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "427dabf5-bd03-4c63-b6ef-4147032ecb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to file\n",
    "out = np.column_stack((E1,E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bbf9d15-e7dc-40aa-b160-4764fbd05703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelNames = 'Source,Target'\n",
    "np.savetxt(savename,out,fmt=('%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "461c7ebd-3091-4cb4-95c5-73333742d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights too\n",
    "np.savetxt(savename2,np.column_stack((out,E['weight'])),fmt=('%d','%d','%d'),delimiter='\\t',comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728c1b8-6ca7-49c8-beef-c0a17f048196",
   "metadata": {},
   "source": [
    "## k-Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bad5166a-7094-4d59-b49a-a3006d769e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from snap import GenPrefAttach,SaveEdgeList,TRnd\n",
    "import subprocess\n",
    "\n",
    "def getDegreeList(A):\n",
    "    # n = np.unique(np.vstack((A[:,0],A[:,1]))).shape[0]\n",
    "    n = int(np.max(np.vstack((A[:,0],A[:,1]))) + 1)\n",
    "    degreeVec = np.zeros(n,dtype=int)\n",
    "    for e in range(A.shape[0]):\n",
    "        degreeVec[int(A[e,0])] += 1\n",
    "        degreeVec[int(A[e,1])] += 1\n",
    "    return degreeVec\n",
    "\n",
    "def makeWeightedEdgelist(A,outname):\n",
    "    #still remove self loops, as they make no sense in this context\n",
    "    Atmp = np.array([row for row in A if row[0] != row[1]])\n",
    "    inds = np.lexsort((Atmp[:,1],Atmp[:,0]))\n",
    "    Asort = Atmp[inds,:]\n",
    "    #get number of unique entries by taking diff\n",
    "    Adiff1 = np.vstack((np.array([1,1]),np.diff(Asort,axis=0)))\n",
    "    Adiff = np.any(Adiff1!=0,axis=1)\n",
    "    #find where the diffs are equal to 1 a and diff that to get counts of unique\n",
    "    outUnique = Asort[Adiff==1]\n",
    "    outCounts = np.diff(np.hstack((np.where(Adiff==1)[0],Adiff.shape[0])))\n",
    "    out = np.column_stack((outUnique,outCounts))\n",
    "    if outname:\n",
    "        np.savetxt(outname,out,fmt=('%d','%d','%d'),delimiter='\\t',comments='')\n",
    "    return out\n",
    "\n",
    "\n",
    "def removeDuplicateEdges(X):\n",
    "    #remove duplicates and self loops (and also sort)\n",
    "    # xtmp = np.vstack({tuple(row) for row in X})\n",
    "    xtmp = np.vstack({tuple(row) for row in X if row[0] != row[1]})\n",
    "    inds = np.lexsort((xtmp[:,1],xtmp[:,0]))\n",
    "    out = xtmp[inds,:]\n",
    "    return out\n",
    "        \n",
    "\n",
    "def myPA(nodes,m,seed=4639):\n",
    "    np.random.seed(seed)\n",
    "    edgeList = []\n",
    "    degreeVec = np.zeros(nodes)\n",
    "    #initialize first step\n",
    "    degreeVec[0:2] = np.array([1, 1])\n",
    "    edgeList.append((0,1))\n",
    "    for n in np.arange(2,nodes):\n",
    "        #connect to existing vertices according to preferential attachment model\n",
    "        # weighting of distribution is degreeVec[:n]\n",
    "        probs = np.double(degreeVec[:n])\n",
    "        neighbors = np.random.choice(np.arange(n),m,replace=True,p=probs/np.sum(probs))\n",
    "        # print neighbors\n",
    "        degreeVec[n] = m\n",
    "        for dit in np.arange(m):\n",
    "            #if edge included, increment both degrees and append edge to the list\n",
    "            degreeVec[neighbors[dit]] += 1\n",
    "            edgeList.append((neighbors[dit],n))\n",
    "        # print degreeVec\n",
    "        # print \"avg degree: \" + str(np.sum(degreeVec)/n)\n",
    "    return np.asarray(edgeList)\n",
    "\n",
    "\n",
    "def generateGraphs(params):\n",
    "    graphname = params['graph']\n",
    "    n = int(params['n'])\n",
    "    numit = int(params['numGen'])\n",
    "    graphType = params['type']\n",
    "        \n",
    "    if graphType == 'GNP':\n",
    "        deg = int(params['d'])\n",
    "        #every node has average degree deg, total number of edges is deg*n/2, divide by total possible edges 2/(n*(n-1))\n",
    "        p = float(deg)/(n-1)\n",
    "        # print \"degree is \" + str(p)\n",
    "        np.random.seed(4639)\n",
    "        #generate all randomness at once\n",
    "        pairs = np.array([t for t in combinations(np.arange(n),2)])\n",
    "        ps = np.random.rand(pairs.shape[0],numit) <= p\n",
    "        for it in np.arange(numit):\n",
    "            #keep the edges that are sampled\n",
    "            pairsKeep = pairs[ps[:,it]==1]\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'PA':\n",
    "        deg = int(params['d'])\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            # x = myPA(n, int(deg-2./n), seed=it*4639+5011)\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            # x = myPA(n, int(deg/2.), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            # outname = graphname + '_' + graphType + 'mult_' + str(it) + '.txt'\n",
    "            # makeWeightedEdgelist(x,tmpname)\n",
    "            # np.savetxt(tmpname,x,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            xfinal = removeDuplicateEdges(x)\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            #make a weighted graph, keep track of weights for direct comparison with twilightEdgesIDsWeights.txt\n",
    "            \n",
    "    #keep the top edges that correspond to target |E| in original graph\n",
    "    elif graphType == 'Pthresh':\n",
    "        deg = int(params['d'])\n",
    "        # Etarget = deg*n/2\n",
    "        for it in np.arange(numit):\n",
    "            #is this degree right? or scale by 2\n",
    "            #solve directly: 2/n + 2m = deg = 2|E|/n\n",
    "            x = myPA(n, int(deg/2.-1./n), seed=it*4639+5011)\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_dup.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            #take the Etarget edges with largest weight\n",
    "            Etarget = min(np.floor(deg*n/2.),xweighted.shape[0])\n",
    "            eind = np.argsort(xweighted[:,2])[::-1] #sort by weight\n",
    "            xtop = removeDuplicateEdges(xweighted[eind[:Etarget],:2])\n",
    "            np.savetxt(outname,xfinal,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            \n",
    "\n",
    "    elif graphType == 'PAsnap':\n",
    "        deg = int(params['d'])\n",
    "        Trnd1 = TRnd()\n",
    "        for it in np.arange(numit):\n",
    "            #generate graph\n",
    "            Trnd1.PutSeed(it*4639+5011)\n",
    "            x = GenPrefAttach(n,deg,Trnd1)\n",
    "            #save output\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            SaveEdgeList(x,outname)\n",
    "            #remove the top 3 lines, sed -i '' -e 1,3d tmp.txt\n",
    "            emp = ''\n",
    "            out = subprocess.call([\"sed\", \"-i\", emp, \"-e\", \"1,3d\", outname])\n",
    "            \n",
    "    elif graphType == 'CL':\n",
    "        #get degree sequence from input\n",
    "        w = params['dList']\n",
    "        wnorm = float(np.sum(w))\n",
    "        nc2 = int(n*(n-1)/2)\n",
    "        pairs = np.zeros((nc2,2))\n",
    "        pairComp = np.zeros(nc2)\n",
    "        for e,(i,j) in enumerate(combinations(np.arange(n),2)):\n",
    "            #array comparison\n",
    "            pairComp[e] = w[i]*w[j]/wnorm\n",
    "            pairs[e,0] = i\n",
    "            pairs[e,1] = j\n",
    "        rands = np.random.rand(nc2,numit)\n",
    "        for it in np.arange(numit):\n",
    "                pairsKeep = pairs[rands[:,it] < pairComp]\n",
    "                outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "                np.savetxt(outname,pairsKeep,fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "\n",
    "    elif graphType == 'CNFG':\n",
    "        w = params['dList']\n",
    "        wnorm = int(np.sum(w))\n",
    "        elist = np.zeros(wnorm)\n",
    "        st = 0\n",
    "        for i,wi in enumerate(w):\n",
    "            elist[st:(st+wi)] = i\n",
    "            st += wi\n",
    "        for it in np.arange(numit):\n",
    "            plist = np.random.permutation(elist)\n",
    "            x = plist.reshape(-1,2)\n",
    "            #if column 1 is greater than column 0 then swap that column\n",
    "            xswap = x[:,0] > x[:,1]\n",
    "            x[xswap,0:2] = np.column_stack((x[xswap,1],x[xswap,0]))\n",
    "            tmpname = graphname + '_' + graphType + '_' + str(it) + '_wt.txt'\n",
    "            outname = graphname + '_' + graphType + '_' + str(it) + '.txt'\n",
    "            #sort correctly and remove self loops, duplicates\n",
    "            xweighted = makeWeightedEdgelist(x,tmpname)\n",
    "            np.savetxt(outname,xweighted[:,:2],fmt=('%d','%d'),delimiter='\\t',comments='')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    #example parameters\n",
    "    #123 undirected, but 1031 total weight if including multiedges\n",
    "#     params = {'graph': 'twilight','type':'PA','n': 27,'d': int(2*1031/27),'numGen': 3}\n",
    "    params = {'graph': 'twilight','type':'CL','n': 3,'dList': [int(2*1031/27), 72, 76],'numGen': 3}\n",
    "\n",
    "    # params = {'graph': 'twilight','type':'PA','n': 27,'d': int(2*123/27),'numGen': 3}\n",
    "    # 575 undirected, but 9464 total weight if including multiedges\n",
    "#     params = {'graph': 'goblet','type':'PA','n': 62,'d': int(2*575/62),'numGen': 3}\n",
    "    generateGraphs(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbbd19-cd4f-47f1-a58b-d701694b5742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61746c3c-ae2b-4247-8745-7abe8c2151d1",
   "metadata": {},
   "source": [
    "# Modelling Character Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774da8d3-23b2-49b5-8f76-ce6b0b9a3313",
   "metadata": {},
   "source": [
    "## Preferential Attachement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "53e6d05b-1a11-4ac3-9133-12e579e1c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import io\n",
    "from snap import GenPrefAttach,SaveEdgeList,TRnd\n",
    "from sklearn import svm, base, feature_selection, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve,auc,classification_report,f1_score,accuracy_score,roc_auc_score\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import subprocess\n",
    "# from generateGraphsMain import *\n",
    "from os import listdir\n",
    "\n",
    "#to visualize\n",
    "from six import StringIO\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc111d5d-270d-4d97-b62a-a893ed16a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random graphs...\n",
      "initializing directory and taking features of original graph...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['mkdir', 'graphs']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-34f43a7efe36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m#automate initializing directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m#this will throw an error if there is already a folder named graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0minitializeDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-34f43a7efe36>\u001b[0m in \u001b[0;36minitializeDirectory\u001b[0;34m(origGraph)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m#compute graph profile features using GraphLab PowerGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initializing directory and taking features of original graph...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mkdir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graphs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#graph\\tsample_prob_keep\\tn3_3\\tn3_2\\tn3_1\\tn3_0\\tn4_0\\tn4_1\\tn4_2\\tn4_3\\tn4_4\\tn4_5\\tn4_6\\tn4_7\\tn4_8\\tn4_9\\tn4_10\\truntime\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'counts_4_profilesLocal.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    416\u001b[0m                **kwargs).stdout\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['mkdir', 'graphs']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "def doSVMcvPrediction(Xin,yin,Xtest,ytest,modType):\n",
    "    #input: training/test data and labels, model type\n",
    "    #supported models: SVM (l1 and l2), AdaBoost, decision tree, and random forest\n",
    "    #train with 5-fold cross validation, then test once using test (holdout) data\n",
    "    #once the best estimator is chosen here, train on the entire dataset (in + test) outside this function\n",
    "    #output: training accuracy, gereralization accuracy, feature weights/importances, classifier, \n",
    "    #  classification report, training f1-score and generalization f1-score\n",
    "    nfolds = 5\n",
    "    cv = cross_val_score(yin,nfolds,shuffle=True)\n",
    "    #l1 penalty enforces sparsity in weights, only available for linear SVM classifier\n",
    "    if modType in ('SVM-L2','svm-l2'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l2', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            # scoring='roc_auc',\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('SVM-L1','svm-l1'):\n",
    "        clasf = svm.LinearSVC(loss='squared_hinge', penalty='l1', tol=.001, dual=False, class_weight='balanced')\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'C' : [0.05, 0.1, 0.5, 1, 5, 10, 500, 1000]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "    \n",
    "    #decision tree classifiers\n",
    "    elif modType in ('ada','adaboost','adaboost-tree'):\n",
    "        clasf = AdaBoostClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'learning_rate' : [0.1,0.3,0.5]\n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('dtree','decision-tree'):\n",
    "        clasf = DecisionTreeClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'splitter' : ['best'],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "\n",
    "    elif modType in ('rf','random-forest'):\n",
    "        clasf = RandomForestClassifier()\n",
    "        cvclasf = GridSearchCV(clasf, param_grid = {\n",
    "            'n_estimators' : [5,10,25,50,100],\n",
    "            'criterion' : ['entropy','gini'],\n",
    "            'max_features' : [0.2,'sqrt',1.],\n",
    "            'max_depth' : [2,4], \n",
    "            'class_weight' : ['balanced'], \n",
    "            }, verbose=0,refit=True,\n",
    "            cv=cv,\n",
    "            scoring='f1_weighted',\n",
    "        n_jobs=4)\n",
    "        \n",
    "    #TODO: add linear regression, logistic regression, etc. \n",
    "\n",
    "    cvclasf.fit(Xin,yin)\n",
    "    bclasf = cvclasf.best_estimator_\n",
    "    print(\"%s %d-fold CV params: %s\" % (modType,nfolds,cvclasf.best_params_))\n",
    "    \n",
    "    if modType in ('ada','adaboost-tree','dtree','decision-tree','rf','random-forest'):\n",
    "        w = bclasf.feature_importances_\n",
    "    elif modType in ('SVM-L1','svm-l1','SVM-L2','svm-l2'):\n",
    "        w = bclasf.coef_\n",
    "    \n",
    "    bclasf.fit(Xin,yin)\n",
    "    y_train_pred = bclasf.predict(Xin)\n",
    "    acTrain = accuracy_score(yin,y_train_pred)\n",
    "    f1Train = f1_score(yin,y_train_pred,average=\"weighted\")\n",
    "    \n",
    "    y_pred = bclasf.predict(Xtest)\n",
    "    report = classification_report(ytest, y_pred)\n",
    "    acGeneral = accuracy_score(ytest, y_pred)\n",
    "    f1Gen = f1_score(ytest,y_pred,average=\"weighted\")\n",
    "\n",
    "    return(acTrain,np.squeeze(w),bclasf,report,(acTrain,acGeneral),(f1Train,f1Gen))\n",
    "\n",
    "def initializeDirectory(origGraph):\n",
    "    #compute graph profile features using GraphLab PowerGraph\n",
    "    print(\"initializing directory and taking features of original graph...\")\n",
    "    out = subprocess.check_output([\"mkdir\", \"graphs\"])\n",
    "    hdr = \"#graph\\tsample_prob_keep\\tn3_3\\tn3_2\\tn3_1\\tn3_0\\tn4_0\\tn4_1\\tn4_2\\tn4_3\\tn4_4\\tn4_5\\tn4_6\\tn4_7\\tn4_8\\tn4_9\\tn4_10\\truntime\\n\"\n",
    "    with open('counts_4_profilesLocal.txt', 'w') as fpt:\n",
    "        fpt.write(hdr)\n",
    "    pcommand = '/Users/vatsalpatel'\n",
    "    out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", origGraph])\n",
    "    hdr2 = \"#graph\\tevbin0\\tevbin1\\tevbin2\\tevbin3\\tevbin4\\n\"\n",
    "    with open('counts_eval_bins.txt', 'w') as fpt:\n",
    "        fpt.write(hdr2)\n",
    "    generateEigenvalueBins(origGraph,\"counts_eval_bins.txt\")\n",
    "    return 0\n",
    "\n",
    "def generateEigenvalueBins(gname,outDir,nbins=5):\n",
    "    #get normalized laplacian\n",
    "    hbins = np.histogram(np.array([0,2]),bins=nbins)[1]\n",
    "    E = np.loadtxt(gname,delimiter='\\t')\n",
    "    #map everything to number of unique vertices\n",
    "    un = np.unique(np.vstack((E[:,0],E[:,1])))\n",
    "    n = len(un)\n",
    "    A = np.zeros((n,n))\n",
    "    for e in np.arange(E.shape[0]):\n",
    "        tmp0 = np.argwhere(un==E[e,0])\n",
    "        tmp1 = np.argwhere(un==E[e,1])\n",
    "        A[tmp0,tmp1] = 1\n",
    "        A[tmp1,tmp0] = 1\n",
    "    D = np.diag(np.sum(A,1))\n",
    "    Di = np.linalg.inv(np.sqrt(D))\n",
    "    L = np.eye(n) - Di.dot(A).dot(Di)\n",
    "    teigs = np.linalg.eigvalsh(L)\n",
    "    #take histogram\n",
    "    neig = len(teigs.flatten()) + nbins\n",
    "    nh = np.histogram(teigs,bins=hbins)[0]    \n",
    "    ep = (nh + 1.)/neig #add smoothing and normalize\n",
    "    #append to file\n",
    "    with open(outDir, \"a\") as myfile:\n",
    "        myfile.write(gname + \"\\t\" +  \"\\t\".join([str(e) for e in ep]) + \"\\n\")\n",
    "    return 0\n",
    "\n",
    "def writeTree(treeModel,namesList,filename):\n",
    "    #utility function that plots a decision tree and saves to file\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(treeModel,out_file=dot_data,feature_names=namesList)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(filename) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    genData = 1 #flag to generate random graphs\n",
    "    classify = 0 #flag to classify\n",
    "    useSpectral = 0 #flag to include eigenvalue histograms\n",
    "\n",
    "    if genData:\n",
    "\n",
    "        print(\"generating random graphs...\")\n",
    "        # #generate graphs, these parameters should be easy to automate\n",
    "        # n,E,filename,outname = 27,123,'./twilightEdgesIDsWeights.txt','graphs/twilight'\n",
    "        # n,E,filename,outname = 27,1031,'./twilightEdgesIDsWeights.txt','graphs/twilight'\n",
    "        n,E,filename,outname = 39,280,'./thestandEdgesIDsWeights.txt','graphs/thestand'\n",
    "        # n,E,filename,outname = 39,6539,'./thestandEdgesIDsWeights.txt','graphs/thestand'\n",
    "        # n,E,filename,outname = 62,575,'./gobletEdgesIDsWeights.txt','graphs/goblet'\n",
    "        # n,E,filename,outname = 62,9464,'./gobletEdgesIDsWeights.txt','graphs/goblet'\n",
    "        \n",
    "        #automate initializing directory\n",
    "        #this will throw an error if there is already a folder named graphs\n",
    "        initializeDirectory(filename)\n",
    "\n",
    "        A = np.loadtxt(filename,delimiter='\\t')\n",
    "        degreeVec = getDegreeList(A)\n",
    "\n",
    "        params = {'graph': outname,'type':'CNFG','n': n,'dList': degreeVec,'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        params = {'graph': outname,'type':'CL','n': n,'dList': degreeVec,'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        \n",
    "        # params = {'graph': outname,'type':'PA','n': n,'d': int(2*E/n),'numGen': 50}\n",
    "        params = {'graph': outname,'type':'PA','n': n,'d': int(2*E/n),'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        params = {'graph': outname,'type':'GNP','n': n,'d': int(2*E/n),'numGen': 100}\n",
    "        generateGraphs(params)\n",
    "        # #generate 100 graphs each from 3 classes in < 3 seconds\n",
    "        #thresholded PA model\n",
    "       \n",
    "\n",
    "        graph_dir = '/Users/vatsalpatel/Documents/novels/graphs/'\n",
    "        #feature design 1\n",
    "        #analyze global 4-profiles\n",
    "        #separate by generative model?\n",
    "        # /Users/ethan/graphlab-master-2/release/apps/4-profiles/4profile --format tsv --graph /Users/ethan/Documents/novels/twilightEdgesIDs.txt\n",
    "        print(\"computing global subgraph counts...\")\n",
    "        pcommand = '/Users/vatsalpatel'\n",
    "        for gname in listdir(graph_dir):\n",
    "            if gname.endswith('.txt') and \"_wt\" not in gname:\n",
    "            # if \"_CL_\" in gname:\n",
    "                # out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", gname, \"--per_vertex\", gname])\n",
    "                out = subprocess.check_output([pcommand, \"--format\", \"tsv\", \"--graph\", graph_dir + gname])\n",
    "                if useSpectral:\n",
    "                    generateEigenvalueBins(graph_dir+gname,\"counts_eval_bins.txt\")\n",
    "        out = subprocess.check_output([\"mv\", \"counts_4_profilesLocal.txt\", \"graphs\"])\n",
    "        out = subprocess.check_output([\"mv\", \"counts_eval_bins.txt\", \"graphs\"])\n",
    "        #global 4 profiles for 100 graphs each from 3 classes in ~1 minute\n",
    "\n",
    "        # additional feature design could include distributions of local 4-profiles throughout graph\n",
    "        # or pagerank or centrality measures\n",
    "        \n",
    "    if classify:\n",
    "        #build classifiers\n",
    "        #as a baseline, split data into train and test \n",
    "        #this will verify the classifier can differentiate between graph families\n",
    "        \n",
    "        np.random.seed(423322) #for repeatability during writeup\n",
    "        graphFolder = 'graphsGoblet/'\n",
    "        # graphFolder = 'graphsTwilight/'\n",
    "        # graphFolder = 'graphsTheStand/'\n",
    "        print(\"Reading data from folder %s\" % graphFolder)\n",
    "\n",
    "        #read data from 4-profile output file, read labels and split into train and test\n",
    "        featInds = np.arange(2,17)\n",
    "        D = pd.read_csv(graphFolder + 'counts_4_profilesLocal.txt',delimiter='\\t')\n",
    "        X = np.array(D.ix[1:,featInds])\n",
    "        # print X[0,:]\n",
    "        #add eigenvalue histogram to X\n",
    "        if useSpectral:\n",
    "            #assume its in the exact same order\n",
    "            featInds2 = np.arange(1,6) #5 bins\n",
    "            D2 = pd.read_csv(graphFolder + 'counts_eval_bins.txt',delimiter='\\t')\n",
    "            X = np.hstack((X,np.array(D2.ix[1:,featInds2])))\n",
    "            # X = np.array(D2.ix[1:,featInds2])\n",
    "        # print X[0,:]\n",
    "        y = np.zeros(X.shape[0])\n",
    "        # D.ix[D['#graph'].str.contains('CL'),2:17]\n",
    "        RGfamilies = ['CL','GNP','PA','CNFG']\n",
    "        # RGfamilies = ['CL','GNP','PA','PAmult']\n",
    "        for i,s in enumerate(RGfamilies):\n",
    "        #     print(i)\n",
    "            y[np.array(D['#graph'].ix[1:].str.contains(s))] = i\n",
    "        # print y\n",
    "        holdfrac = 0.5\n",
    "        Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=holdfrac,stratify=y)\n",
    "\n",
    "        #preprocess\n",
    "        scaler1=StandardScaler()\n",
    "        Xtrain = scaler1.fit_transform(Xtrain.astype(np.double))\n",
    "        Xtest = scaler1.transform(Xtest.astype(np.double))\n",
    "        \n",
    "        # modelType = 'SVM-L2'\n",
    "        # modelType = 'SVM-L1'\n",
    "        # modelType = 'adaboost-tree'\n",
    "        modelType = 'decision-tree'\n",
    "        # modelType = 'random-forest'\n",
    "        score,optWeights,clasf,rep,accs,f1s = doSVMcvPrediction(Xtrain, ytrain, Xtest, ytest, modelType)\n",
    "        #classifier works perfectly\n",
    "        # print clasf.get_params\n",
    "        print(\"Checking distinctness of random graph families...\")\n",
    "        print(rep)\n",
    "        # print accs[0],accs[1],f1s[0],f1s[1]\n",
    "        # print np.column_stack((optWeights.T,D.columns[featInds]))\n",
    "        \n",
    "        #see which random graph model the novel gets classified as\n",
    "        x = np.array(D.ix[0,featInds]).reshape(1,-1)\n",
    "        if useSpectral:\n",
    "            x = np.hstack((x,np.array(D2.ix[0,featInds2]).reshape(1,-1)))\n",
    "            # x = np.array(D2.ix[0,featInds2]).reshape(1,-1)\n",
    "        scaler=StandardScaler()\n",
    "        X = scaler.fit_transform(X.astype(np.double))\n",
    "        x = scaler.transform(x.astype(np.double)) \n",
    "        clasf.fit(X,y)\n",
    "        novelRG = clasf.predict(x)\n",
    "        if useSpectral:\n",
    "            Fcol = D.columns[featInds].append(D2.columns[featInds2])\n",
    "            # Fcol = D2.columns[featInds2]\n",
    "        else:\n",
    "            Fcol = D.columns[featInds]\n",
    "        # print Fcol\n",
    "        if modelType in ('SVM-L1','SVM-L2'):\n",
    "            #TODO: print statements when useSpectral\n",
    "            print(\"SVM feature weights:\")\n",
    "            print(np.column_stack((clasf.coef_.T,Fcol)))\n",
    "            sc = x.dot(clasf.coef_.T) + clasf.intercept_\n",
    "            print(\"scores:\")\n",
    "            # print np.vstack((RGfamilies,(sc-np.min(sc))/np.sum(sc-np.min(sc))))\n",
    "            print(np.vstack((RGfamilies,sc)))\n",
    "        elif modelType in ('adaboost-tree'):\n",
    "            print(\"Adaboost feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            # print \"prediction probabilities:\" \n",
    "            # print np.vstack((RGfamilies,clasf.predict_proba(x)))\n",
    "            print(\"decision function:\")\n",
    "            print(np.vstack((RGfamilies,clasf.decision_function(x))))\n",
    "            #also the actual tree?\n",
    "        elif modelType in ('decision-tree'):\n",
    "            print(\"Decision tree feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            print(\"prediction probabilities:\" )\n",
    "            print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "            #also the actual tree\n",
    "            Fcol2 = [s.replace('n3','H').replace('n4','F') for s in Fcol]\n",
    "            writeTree(clasf,Fcol2,graphFolder[:-1]+'_dTree.pdf')\n",
    "        elif modelType in ('random-forest'):\n",
    "            print(\"Random Forest feature weights:\")\n",
    "            print(np.column_stack((clasf.feature_importances_.T,Fcol)))\n",
    "            print(\"prediction probabilities:\" )\n",
    "            print(np.vstack((RGfamilies,clasf.predict_proba(x))))\n",
    "        #print the prediction (and confidence score?)\n",
    "        print(\"Fiction novel classified as: %s\" % RGfamilies[int(novelRG)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6661c-c528-443e-bbcb-f7a0c32c6ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626ff71-34ae-4731-94ed-a994a17ffc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c969f59f-cdcd-4abd-a919-efb3780667bd",
   "metadata": {},
   "source": [
    "## Chung-Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed6ad2-5288-4a0c-942a-b0ff174fa75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd00bfe-59e4-486c-b74d-a94aa111b414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dabb369a-9472-4dea-8a3b-c6a0b733316d",
   "metadata": {},
   "source": [
    "## Binomial Random Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37044e0-e667-4ec4-89f4-26a253bceb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239da31-75c0-4fd0-b3f8-b275ca548dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e53bcb-61ce-43a2-a180-be057aa922e5",
   "metadata": {},
   "source": [
    "## Configuration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57c92d-d365-4a01-ae7f-51307118dc93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a602cf-c81e-4101-a753-744be6787cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42599d25-12bb-4a27-8a67-73875b6197b0",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9223e9e-b820-4ca5-8b75-fb3194bb0324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aba390-2cc6-47ab-a3a9-f8e9a71157ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3de60b-923c-4168-88b9-36aaf2ca1454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
